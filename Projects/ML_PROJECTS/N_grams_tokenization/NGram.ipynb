{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Importing necessary libraries"
      ],
      "metadata": {
        "id": "7GdBAspsVkVp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HxeYhvMVRws"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Needed to download the corpus zip as was not already available"
      ],
      "metadata": {
        "id": "WjsLRerqYyhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0wsuGpaYxdO",
        "outputId": "80dcb08b-8db2-447a-801a-4ae91bbd0152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Processing the corpus after initialization"
      ],
      "metadata": {
        "id": "a0en7NfYVqyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading corpus to have collection of words\n",
        "corpus = brown.words()\n",
        "\n",
        "#Lower casing the corpus words for uniformity and using set to get the unique words in vocabulary\n",
        "lower_case_corpus = [w.lower() for w in corpus]\n",
        "vocab = set(lower_case_corpus)\n",
        "\n",
        "print(lower_case_corpus[:30])\n",
        "print(list(vocab)[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrXfIUJXVeRo",
        "outputId": "c8bfe97d-4b12-499e-f095-618dcf671345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.', 'the', 'jury', 'further', 'said', 'in']\n",
            "['wholeness', \"admassy's\", '1610', 'baltimorean', 'avidly', 'elegies', 'inveterate', 'superstructure', 'expectancy', 'snakestrike', 'apportionments', '$31,179,816', 'privileges', 'roslev', 'condensation', 'self-rule', 'hitherto', 'ourselves', \"7''\", 'legislate', 'modern-dance', 'bartha', 'statuto', 'television-electronics', 'tracers', 'harrington', 'locked', 'earrings', 'focally', 'sepia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Printing the total size of the corpus and the vocaulary obtained from the corpus"
      ],
      "metadata": {
        "id": "LMzTJKR-Z46e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total words in corpus: {(len(lower_case_corpus))}\")\n",
        "print(f\"Total vocab size: {len(vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf0qep9cYRJF",
        "outputId": "e5277e02-cbed-46f3-b172-1083ec891625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in corpus: 1161192\n",
            "Total vocab size: 49815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setting the value of n for the n grams language modelling"
      ],
      "metadata": {
        "id": "aXAdeBIiZ4Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1 for unigram, 2 for bigram & 3 for trigram\n",
        "n=3"
      ],
      "metadata": {
        "id": "twciSSDiZ1N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Populating the ngrams_counts set according to the n value to get frequency if each set of n words\n"
      ],
      "metadata": {
        "id": "ctOh0a_jhdFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_counts = {}\n",
        "n_minus1_gram_counts = {}\n",
        "\n",
        "for i in range(len(lower_case_corpus) - n + 1):\n",
        "    ngram = tuple(lower_case_corpus[i:i+n])\n",
        "    n_minus1_gram = ngram[:-1]\n",
        "\n",
        "    if ngram in ngram_counts:\n",
        "        ngram_counts[ngram] += 1\n",
        "    else:\n",
        "        ngram_counts[ngram] = 1\n",
        "\n",
        "    if n_minus1_gram in n_minus1_gram_counts:\n",
        "        n_minus1_gram_counts[n_minus1_gram] += 1\n",
        "    else:\n",
        "        n_minus1_gram_counts[n_minus1_gram] = 1\n",
        "\n",
        "print(list(ngram_counts)[:20])\n",
        "print(list(n_minus1_gram_counts)[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kQy2A-_YrEA",
        "outputId": "295b149f-b633-41cd-b54f-825294e1cf22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 'fulton', 'county'), ('fulton', 'county', 'grand'), ('county', 'grand', 'jury'), ('grand', 'jury', 'said'), ('jury', 'said', 'friday'), ('said', 'friday', 'an'), ('friday', 'an', 'investigation'), ('an', 'investigation', 'of'), ('investigation', 'of', \"atlanta's\"), ('of', \"atlanta's\", 'recent'), (\"atlanta's\", 'recent', 'primary'), ('recent', 'primary', 'election'), ('primary', 'election', 'produced'), ('election', 'produced', '``'), ('produced', '``', 'no'), ('``', 'no', 'evidence'), ('no', 'evidence', \"''\"), ('evidence', \"''\", 'that'), (\"''\", 'that', 'any'), ('that', 'any', 'irregularities')]\n",
            "[('the', 'fulton'), ('fulton', 'county'), ('county', 'grand'), ('grand', 'jury'), ('jury', 'said'), ('said', 'friday'), ('friday', 'an'), ('an', 'investigation'), ('investigation', 'of'), ('of', \"atlanta's\"), (\"atlanta's\", 'recent'), ('recent', 'primary'), ('primary', 'election'), ('election', 'produced'), ('produced', '``'), ('``', 'no'), ('no', 'evidence'), ('evidence', \"''\"), (\"''\", 'that'), ('that', 'any')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prediction suggestions of the next words according to user inputs"
      ],
      "metadata": {
        "id": "FFMGAt8-joDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_next_word(input_, ngram_counts, n_minus1_gram_counts, vocab):\n",
        "    # Consider the last n-1 words of the input as the context\n",
        "    tokenized_input = word_tokenize(input_.lower())\n",
        "    last_ngram = tokenized_input[-n+1:]\n",
        "\n",
        "    # Calculating probability for each word in vocab\n",
        "    vocab_probabilities = {}\n",
        "    for vocab_word in vocab:\n",
        "        test_ngram = tuple(last_ngram + [vocab_word])\n",
        "\n",
        "        test_ngram_count = ngram_counts.get(test_ngram, 0)\n",
        "        n_minus1_gram_count = n_minus1_gram_counts.get(tuple(last_ngram), 0)\n",
        "\n",
        "        if n_minus1_gram_count != 0:\n",
        "            probability = test_ngram_count / n_minus1_gram_count\n",
        "        else:\n",
        "            probability = 0\n",
        "\n",
        "        vocab_probabilities[vocab_word] = probability\n",
        "\n",
        "    # Sorting the vocab probability in descending order to get top probable words\n",
        "    top_suggestions = sorted(vocab_probabilities.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "    return top_suggestions\n"
      ],
      "metadata": {
        "id": "PdNcS6vmhr_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing working of function"
      ],
      "metadata": {
        "id": "pUBwoB_ejvhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "suggest_next_word('I am the king',ngram_counts,n_minus1_gram_counts,vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTSNDPtPjnrM",
        "outputId": "faf38004-2931-42c4-c637-74c01f1e2d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('james', 0.17647058823529413),\n",
              " ('of', 0.1568627450980392),\n",
              " ('arthur', 0.11764705882352941)]"
            ]
          },
          "metadata": {},
          "execution_count": 304
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "suggest_next_word('The fulton county',ngram_counts,n_minus1_gram_counts,vocab)"
      ],
      "metadata": {
        "id": "HjkypaG-m6v6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41ffa726-827b-44c2-a512-cf7ea4fb3ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('grand', 0.16666666666666666),\n",
              " ('jail', 0.16666666666666666),\n",
              " ('general', 0.16666666666666666)]"
            ]
          },
          "metadata": {},
          "execution_count": 305
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "suggest_next_word('I am the king of',ngram_counts,n_minus1_gram_counts,vocab)"
      ],
      "metadata": {
        "id": "W-zCqwNZpPfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b81487c4-943c-4975-b3c9-c32f98ac000f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('france', 0.3333333333333333),\n",
              " ('hearts', 0.16666666666666666),\n",
              " ('orators', 0.08333333333333333)]"
            ]
          },
          "metadata": {},
          "execution_count": 306
        }
      ]
    }
  ]
}